{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18\n",
    "#https://www.youtube.com/watch?v=0bt0SjbS3xc&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cartpole1 import QLearnCartPoleSolver\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torchvision\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "        dev = \"cuda:0\" \n",
    "    else: \n",
    "        dev = \"cpu\" \n",
    "    return torch.device(dev)\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, height, width):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=height*width*3, out_features=24), \n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=24, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQNSolver(QLearnCartPoleSolver):\n",
    "\n",
    "    def __init__(self, env,  episodes, device, epsilon_decay_rate=0.995):\n",
    "        super().__init__(env, episodes=episodes, min_epsilon=0.001)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.epsilon = 1\n",
    "        self.batch_size = 64\n",
    "        self.device = device\n",
    "        self.lr = 0.01\n",
    "        self.model = DQN().to(self.device)\n",
    "        self.optimizer = torch.optim.RMSprop(self.model.parameters())\n",
    "\n",
    "    def action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon: \n",
    "            return torch.tensor(self.env.action_space.sample()).to(self.device)\n",
    "        else:\n",
    "            with torch.no_grad(): \n",
    "                self.model.predict(state).argmax(dim =1).to(self.device)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def updated_q_value(self, state, action, reward, new_state):\n",
    "        return (reward + self.discount * self.model.predict(state).argmax(dim =1).to(self.device))\n",
    "\n",
    "\n",
    "    def create_batch_tensor(self,batch):\n",
    "        states, actions, rewards, next_states = batch\n",
    "        return torch.tensor(states) ,torch.tensor(actions), torch.tensor(rewards), torch.tensor(next_states)\n",
    "\n",
    "    def replay(self):\n",
    "        if self.batch_size <= len(self.memory):\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states = self.create_batch_tensor(batch)\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            reward_current_ep = 0\n",
    "            step = 1\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.action(state, self.get_epsilon(episode))\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                next_state =  self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                reward_current_ep += reward\n",
    "                step +=1\n",
    "            self.replay()\n",
    "        self.env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
